{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Notes - Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOkakK0aLNzSqN8DQ8Pnti3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jincy-p-janardhanan/ML-Notes/blob/main/ML_Notes_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of contents\n",
        "\n",
        "1. [Artificial neuron](#scrollTo=Artificial_neuron)\n",
        "2. [Neural network](#scrollTo=Neural_network)\n",
        "  1. [A simple neural network with 3 layers](#scrollTo=A_simple_neural_network_with_3_layers)\n",
        "    1. [Hypothesis](#scrollTo=Hypothesis)\n",
        "    2. [Forward Propagation](#scrollTo=Forward_Propagation)\n",
        "    3. [Dimensional Analysis](#scrollTo=Dimensional_Analysis)\n",
        "        - [Example: 4-layer neural network](#scrollTo=Example_4_layer_neural_network)\n",
        "    4. [Implementational note: Problem of symmetric weights](#scrollTo=Implementational_note_Problem_of_symmetric_weights) \n",
        "  2. [Neural networks for classification](#scrollTo=Neural_networks_for_classification)\n",
        "3. [Cost function or Loss function, $\\small J(\\Theta)$](#scrollTo=Cost_function_or_Loss_function_small_J_Theta_small_J_Theta_)\n",
        "4. [Cost optimization (or Model Training)](#scrollTo=Cost_Optimization_or_Model_Training_)\n",
        "  1. [Optmizer and backpropagation](#scrollTo=Optimizer_and_backpropagation)\n",
        "  2. [Gradient of cost function of a neural network](#scrollTo=Gradient_of_cost_function_of_a_neural_network)\n",
        "    1. [Recurrence of $ \\small \\delta $ and $ \\small \\Delta $](#scrollTo=Recurrence_of_small_delta_small_delta_and_small_Delta_small_Delta_)\n",
        "        1. [Dimensional analysis for $\\small \\delta$](#scrollTo=Dimensional_analysis_for_delta_delta_)\n",
        "        2. [Dimensional analysis for $\\small \\Delta$](#scrollTo=Dimensional_analysis_for_Delta_Delta_)\n",
        "      2. [Derivative D, of a neural network with a regularized cost function](#scrollTo=Derivative_small_D_small_D_of_a_neural_network_with_a_regularized_cost_function)\n",
        "        1. [The problem of overfitting, and regularization](#scrollTo=The_problem_of_overfitting_and_regularization)\n",
        "          1. [L2 Regularization or Ridge Regression](#scrollTo=L2_Regularization_or_Ridge_Regression)\n",
        "          2. [L1 Regularization or Least Absolute Shrinkage and Selection Operator (LASSO) regression](#scrollTo=L1_Regularization_or_Least_Absolute_Shrinkage_and_Selection_Operator_LASSO_regression)\n",
        "5. [Different types of cost functions](#scrollTo=Different_types_of_cost_functions)\n",
        "  1. [Mean Squared Error Cost Function (MSE)](#scrollTo=Mean_Squared_Error_Cost_Function_MSE_)\n",
        "    - [Root Mean Squared Error Cost Function (RMSE or RMS)](#scrollTo=Root_Mean_Squared_Error_Cost_Function_RMSE_or_RMS_)\n",
        "  2. [Cross Entropy Loss](#scrollTo=Cross_Entropy_Loss)\n",
        "    1. [Binary Cross Entropy Loss Function](#scrollTo=Binary_Cross_Entropy_Loss_Function)\n",
        "    2. [Categorical Cross Entropy Loss Function](#scrollTo=Categorical_Cross_Entropy_Loss_Function)\n",
        "6. [Different types of optmizers](#scrollTo=Different_types_of_optimizers)\n",
        "  1. [Gradient Descent](#scrollTo=Gradient_Descent)\n",
        "    1. [Variations of gradient descent for non-convex cost functions](#scrollTo=Variations_of_gradient_descent_for_non_convex_cost_functions)\n",
        "        - [Gradient descent intuitions](#scrollTo=Gradient_descent_intuitions)\n",
        "7. [Appendix](#scrollTo=Appendix)\n",
        "  1. [Why do we have to make the output of a neuron non-linear?](#scrollTo=Why_do_we_have_to_make_the_output_of_a_neuron_non_linear_)\n",
        "  2. [List of all equations](#scrollTo=List_of_all_equations)"
      ],
      "metadata": {
        "id": "45JLSXUwAHCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes on Neural Networks"
      ],
      "metadata": {
        "id": "I7GrbeyMa_TW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Artificial neuron\n",
        "\n",
        "- An artificial neuron represents a single unit of computation.\n",
        "- Given an input $\\small x$, and its parameters (otherwise called weights), represented by $\\small \\Theta$, <br> \n",
        "a neuron performs a linear operation,\n",
        "$$\n",
        "\\small z = \\small \\Theta^T x + b \n",
        "$$ \n",
        "and then makes the output non-linear ([why?](#scrollTo=Why_do_we_have_to_make_the_output_of_a_neuron_non_linear_)) by passing it to a non-linear function $\\small g$, called the **activation function**. \n",
        "$$ \n",
        "\\small a = \\small g(z)  \n",
        "$$\n",
        "- The output of the function $\\small g$, represented by $\\small a $ is  called the **activation of a neuron**.\n",
        "- The activation, $\\small a $, is the output of the neuron and is a measure of some feature of $\\small x $ computed by the neuron.\n",
        "- The term $b$ is called the **bias** of the neuron, and is some real valued constant, $\\small b \\in ℝ$. <br> \n",
        "Usually we assign $\\small b = 1$, for simplicity of computation.\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "<figure>\n",
        "  <img src=\"https://i.ibb.co/sV9szcq/neuron.png\"></img>\n",
        "  <figcaption><b>Fig. 1. Artificial neuron</b></figcaption>\n",
        "</figure></img>\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "<b>Additional footnotes:</b><br>\n",
        "$\\small A^T $ means transpose of $\\small A$."
      ],
      "metadata": {
        "id": "_z4_-Pt2HhX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The figure shown above represents a neuron with <br> \n",
        "$ \\small \n",
        "\\text{input, } x = \n",
        "\\begin{bmatrix} \n",
        "x_1 \\\\ \n",
        "x_2 \\\\ \n",
        "x_3 \n",
        "\\end{bmatrix},$ <br>\n",
        "\n",
        "$ \\small \n",
        "\\text{weights, } \\Theta = \n",
        "\\begin{bmatrix} \n",
        "\\Theta_1 \\\\ \n",
        "\\Theta_2 \\\\ \n",
        "\\Theta_3 \n",
        "\\end{bmatrix}, $ <br>\n",
        " \n",
        "$\\small \\text{and bias, b.}$ <br><br>\n",
        "\n",
        "The neuron performs the following computations: <br>\n",
        "$ \\small \n",
        "z = \\Theta^T x + b = \n",
        "\\begin{bmatrix} \n",
        "\\Theta_1 & \\Theta_2 & \\Theta_3 \n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix} \n",
        "x_1 \\\\ \n",
        "x_2 \\\\ \n",
        "x_3 \n",
        "\\end{bmatrix} \n",
        "+ b = (\\Theta_1 x_1 + \\Theta_2 x_2 + \\Theta_3 x_3) + b $\n",
        "\n",
        "$\\small a = g(z)$\n"
      ],
      "metadata": {
        "id": "f6BQ_iniRvVh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network"
      ],
      "metadata": {
        "id": "ymWfgXOfeHFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A single neuron can compute only one feature. To obtain multiple features of the data, we need multiple neurons.<br>"
      ],
      "metadata": {
        "id": "2MVCiIQjiILP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have a training example with 3 features,\n",
        "$ \\small x = \n",
        "\\begin{bmatrix} \n",
        "x_{1} \\\\ \n",
        "x_{2} \\\\ \n",
        "x_{3} \n",
        "\\end{bmatrix}$.\n",
        "\n",
        "And we try to obtain two additional features from $ \\small x $; then we will need two neurons that use different sets of parameters.\n",
        "\n",
        "In other words, we apply the same input to different neurons (having different parameters), to obtain different features of the input.\n",
        "\n",
        "Therefore, our parameters $\\small \\Theta $, becomes <br> \n",
        "\n",
        "$ \\small \\Theta = \n",
        "\\begin{bmatrix} \n",
        "\\Theta_{11} & \\Theta_{21} \\\\\n",
        "\\Theta_{12} & \\Theta_{22} \\\\\n",
        "\\Theta_{13} & \\Theta_{23} \n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "Each column in $\\small \\Theta $ corresponds to parameters of one neuron.\n",
        "\n",
        "<div align=\"center\">\n",
        "<figure>\n",
        "  <img src=\"https://i.ibb.co/SnbT6Ym/one-layer-with-2-neurons-acting-on-the-same-input-x-with-3-features.png\"></img>\n",
        "  <figcaption><b>Fig. 2. Two neurons to compute two features from the same training example</b></figcaption>\n",
        "</figure>\n",
        "</div>\n",
        "\n",
        "<br>\n",
        " \n",
        "- Each neuron processes only one training example at a time.\n",
        "- If our training set $\\small X$ has $\\small m$ examples, then we loop over the entire training set $\\small X$ to obtain features from each training example, or use equivalent vectorized implementations.\n",
        "<br> Note that, the size of the training set ($\\small m$), does not affect the computation of a neuron. The neuron takes into account only the number of input features ($\\small n$) of a training example."
      ],
      "metadata": {
        "id": "yhBI85rRiJgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The two neurons above form one layer. <br> \n",
        "Since the input to the neuron is an actual training example from our dataset, this layer is the first **hidden layer**. <br>\n",
        "- $\\small a_1, \\: a_2$ shown in the above figure, form the activations for the first hidden layer; and may be correctly denoted as - <br> \n",
        "$\\small a^{(1)} = \\begin{bmatrix} a^{(1)}_1 \\\\ a^{(1)}_2 \\end{bmatrix}, \\text{where, superscript (1) stands for layer 1.}$\n",
        "- We take $\\small X $, the set of all training examples, as the 0th layer of a neural network and it is called the **input layer**.\n",
        "- To represent that the parameter $\\small \\Theta $ correspond to layer 0, it may be correctly denoted as: <br>\n",
        "$ \\small \\Theta^{(0)} = \n",
        "\\begin{bmatrix} \n",
        "\\Theta^{(0)}_{11} & \\Theta^{(0)}_{21} \\\\\n",
        "\\Theta^{(0)}_{12} & \\Theta^{(0)}_{22} \\\\\n",
        "\\Theta^{(0)}_{13} & \\Theta^{(0)}_{23} \n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "- By convention, and for simplicity of notations, we assign, $\\small a^{(0)} := X$. <br> \n",
        "Also, for ease of computation, we include $\\small b^{(l)}_i$ as the first column of $\\small \\Theta^{(l)}$ and 1's as the first row of input $\\small X$.<br> <br>\n",
        "Now, let's assume that our training set $\\small X$ has only one training example, i.e. \n",
        "$\\small X = \\begin{bmatrix} \n",
        "x_{1}  \\\\\n",
        "x_{2}  \\\\\n",
        "x_{3}  \n",
        "\\end{bmatrix}\n",
        "$\n",
        "<br>\n",
        "Then, $\\small \\Theta^{(0)}$ and $\\small a^{(0)}$ (with an additional row of 1), may be re-written as follows:<br>\n",
        "$ \\small \\Theta^{(0)} = \n",
        "\\begin{bmatrix} \n",
        "b^{(0)}_1 &  b^{(0)}_2 \\\\\n",
        "\\Theta^{(0)}_{11} & \\Theta^{(0)}_{21} \\\\\n",
        "\\Theta^{(0)}_{12} & \\Theta^{(0)}_{22} \\\\\n",
        "\\Theta^{(0)}_{13} & \\Theta^{(0)}_{23} \n",
        "\\end{bmatrix}\n",
        ", \\:\\:\n",
        "a^{(0)} = \n",
        "\\begin{bmatrix} \n",
        "1 \\\\\n",
        "x_{1}  \\\\\n",
        "x_{2}  \\\\\n",
        "x_{3}  \n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix} \n",
        "1 \\\\\n",
        "a^{(0)}_{1}  \\\\\n",
        "a^{(0)}_{2}  \\\\\n",
        "a^{(0)}_{3}  \n",
        "\\end{bmatrix} \n",
        "$ <br><br>\n",
        "Thus, $\\small z^{(1)} $ may be computed as follows:<br>\n",
        "\\begin{align} \n",
        "\\small z^{(1)} = {(\\Theta^{(0)})}^T a^{(0)} &=\n",
        "\\small \n",
        "\\begin{bmatrix} \n",
        "b^{(0)}_1 & \\Theta^{(0)}_{11} & \\Theta^{(0)}_{12} & \\Theta^{(0)}_{13} \\\\ \n",
        "b^{(0)}_2 &\\Theta^{(0)}_{21} & \\Theta^{(0)}_{22} & \\Theta^{(0)}_{23}\\\\\n",
        "\\end{bmatrix} \n",
        "\\begin{bmatrix} \n",
        "1 \\\\\n",
        "a^{(0)}_{1}  \\\\\n",
        "a^{(0)}_{2}  \\\\\n",
        "a^{(0)}_{3}  \n",
        "\\end{bmatrix} \n",
        "\\\\ \\\\ \\small \n",
        " &= \\small\n",
        " \\begin{bmatrix}\n",
        " b^{(0)}_1 + \\Theta^{(0)}_{11}a^{(0)}_{1} + \\Theta^{(0)}_{12}a_{2} + \\Theta^{(0)}_{13}a^{(0)}_{3} \\\\ \n",
        " b^{(0)}_2 + \\Theta^{(0)}_{21}a^{(0)}_{1} + \\Theta^{(0)}_{22}a^{(0)}_{2} + \\Theta^{(0)}_{23}a^{(0)}_{3}\n",
        " \\end{bmatrix}\n",
        " \\\\ \\\\ \\small \n",
        " &= \\small\n",
        " \\begin{bmatrix}\n",
        " z^{(1)}_1  \\\\ \n",
        " z^{(1)}_2\n",
        " \\end{bmatrix}\n",
        " \\end{align}\n",
        "- In a neural network, each layer can have different activation functions. So we denote the activation function of a layer $\\small l$, by ${g^{[l]}}$. (Using square brackets [ ] in the superscript since g is a function and should not be confused with other variables).\n",
        " \\begin{align}\n",
        "\\\\ \\small \\therefore \\: a^{(1)} &= \\small {g^{[1]}}(z^{(1)}) \n",
        "\\\\ \\\\ \\small &= \\small \n",
        "\\begin{bmatrix}\n",
        " {g^{[1]}}(z^{(1)}_1)  \\\\ \n",
        " {g^{[1]}}(z^{(1)}_2)\n",
        " \\end{bmatrix} \n",
        "\\\\ \\\\ &= \\small\n",
        " \\begin{bmatrix}\n",
        " a^{(1)}_1  \\\\ \n",
        " a^{(1)}_2\n",
        " \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Now, if we had m training examples, each with 3 features, i.e. <br>\n",
        "\n",
        "$\\small \\:\\: X = \\begin{bmatrix} \n",
        "x_{01} & x_{11} & x_{21} & ... & x_{m1}  \\\\\n",
        "x_{02} & x_{12} & x_{22} & ... & x_{m2}  \\\\\n",
        "x_{03} & x_{13} & x_{23} & ... & x_{m3}  \\\\ \n",
        "\\end{bmatrix}\n",
        "$, then $\\small z^{(1)}$ becomes <br>\n",
        "\n",
        "$\\small z^{(1)} = \\begin{bmatrix} \n",
        "z^{(1)}_{01} & z^{(1)}_{11} & z^{(1)}_{21} & ... & z^{(1)}_{m1}  \\\\\n",
        "z^{(1)}_{02} & z^{(1)}_{12} & z^{(1)}_{22} & ... & z^{(1)}_{m2}  \\\\ \n",
        "\\end{bmatrix}\n",
        "$ , and $\\small a^{(1)}$ becomes <br>\n",
        "\n",
        "$\\small a^{(1)} = \\begin{bmatrix} \n",
        "a^{(1)}_{01} & a^{(1)}_{11} & a^{(1)}_{21} & ... & a^{(1)}_{m1}  \\\\\n",
        "a^{(1)}_{02} & a^{(1)}_{12} & a^{(1)}_{22} & ... & a^{(1)}_{m2}  \\\\ \n",
        "\\end{bmatrix}\n",
        "$ \n",
        "<br>\n",
        "\n",
        "Note that there is no change in the output of a single neuron, the neuron still *sees* only one example at a time. When the neuron sees the next example, it just adds another column to its activation. However, with a vectorized implementation, this process can be made to execute faster. \n",
        "\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "<b>Additional footnotes:</b><br>\n",
        "1. Note that $\\small b$, may be included as any row in $\\small \\Theta$, but we have to include a row of 1 at the corresponding index in $\\small a$. <br>\n",
        "2. Otherwise, $\\small b$ may be included as any row in $\\small a$, and a row of 1 may be introduced at the corresponding index in $\\small \\Theta$. <br>\n",
        "3. All of the above mentioned implementations are equivalent. However, we usually choose to include $\\small b$'s and 1's as either the first row or the last row, and $\\small b$ is usually included along with the weights (this makes sense, because $\\small b$ is an input and not an output, whereas $\\small a$ is an output.)"
      ],
      "metadata": {
        "id": "HaY7PYBtwDL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generalized, and vectorized equations for $\\small z$ and $\\small a$, for a given layer $\\small l$, may be written as** <br><br>\n",
        "\\begin{align}\n",
        "\\small \\boxed { z^{(l)} = \\small{(\\Theta^{(l-1)})}^Ta^{(l-1)}} \\tag{1} \\\\ \n",
        "\\small \\boxed { \\:\\:\\:\\:\\:\\: a^{(l)} = \\small  {g^{[l]}}(z^{(l)}) \\:\\:\\:\\:\\:} \\tag{2}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "-D9mnjsCpgQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A simple neural network with 3 layers\n",
        "\n",
        "<div align=\"center\">\n",
        "<figure>\n",
        "  <img src=\"https://i.ibb.co/9vrxrdR/Simple-3-layer-NN-bias-and-weights-not-represented.png\"> </img>\n",
        "  <figcaption><b>Fig. 3. A neural network with 3 layers</b></figcaption>\n",
        "</div>\n",
        "\n",
        "For clarity, the notation of weights $\\small \\Theta^{(l)}_{ji}$, are omitted from Fig.3. It may be understood that each arrow from $\\small a^{(l)}_i$ to a neuron, carries the weights associated with it. \n",
        "\n",
        "Since we are using a vectorized implementation, the bias $\\small b^{(l)}_i$ also need not be represented because they are already included in $\\small \\Theta^{(l)}_j$.\n",
        "\n",
        "(Note that, the parameters of a layer  $\\small l, \\: \\Theta^{(l)}$ is indexed as $ \\small \\Theta^{(l)}_{ji}$ here. This indicates we are accessing parameters column-wise, since parameters of a single neuron is given by one column in $\\small \\Theta^{(l)}$.)\n",
        "<br>\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "- The first layer is called the input layer and is counted as layer 0.\n",
        "- The second layer is called the first hidden layer and is counted as layer 1.\n",
        "- The third layer is the last layer of the neural network and hence, is called the output layer of the neural network. It is counted as layer 2.\n",
        "- In a neural network, there can be any number of hidden layers between the input layer and the output layer, each with any number of neurons (≥ 1).\n",
        "\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "<b> Additional footnotes: </b><br>\n",
        "In the Machine Learning course by Prof. Andrew Ng on Coursera, the counting starts from index 1. We are implementing the assignments in Octave programming language, and in Octave, indices of arrays start from 1. Therefore these are notationally consistent. But in many other programming languages, including python, array indexing starts from 0, so I found it more useful to keep such indexing in my notes. "
      ],
      "metadata": {
        "id": "q96l8SkbnX7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "%%HTML\n",
        "<div style=\"float:right; padding:0px 30px;\">\n",
        "<iframe width=\"300\" height=\"400\" src=\"https://www.youtube.com/embed/tNORRcv1SUg?showinfo=0&disablekb=1&loop=1&playlist=tNORRcv1SUg&controls=1&modestbranding=1\" frameborder=\"0\" allowfullscreen></iframe>\n",
        "</div>\n",
        "<div style=\"padding:10px 0px 0px 0px;\">\n",
        "<ul style=\"font-family: 'Roboto', 'Noto', sans-serif; font-size:16px; font-weight=500px; line-height:24px; -webkit-font-smoothing:'anti-aliased'; text-color:'--paper-grey-900'; align:'justify';\">\n",
        "<li> Generally, we use more number of neurons in layers closer to the input layer and gradually decrease the number of neurons as we move towards the output layer. Intuitively, this can be understood like this: <br>\n",
        "<ul>\n",
        "<li> More features means more detailed (or fine-grained) information about the input. At first, we compute very fine grained information from the input. And then, each successive layer summarizes information from the previous layer, and finally makes a prediction at the last layer. \n",
        "<li> Otherwise you can actually imagine it as making a lego house. First we compute very fine grained information about the data, which acts as the building blocks for next layer. Each hidden layer takes activations of the previous layer and build new informations.</br>\n",
        "<br>\n",
        "</ul>\n",
        "<i>(Note: There are a few exceptions to this general rule of thumb. E.g. bottleneck layers, U-net architecture)</i>\n",
        "<li> The number of layers, number of neurons in each layer, activation function for each layer, etc are collectively called as the architecture of the neural network.\n",
        "</ul>\n",
        "</div>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "cellView": "form",
        "id": "R5mye0IP7PnD",
        "outputId": "bdb4862b-e797-4551-b6b5-3680a770899d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"float:right; padding:0px 30px;\">\n",
              "<iframe width=\"300\" height=\"400\" src=\"https://www.youtube.com/embed/tNORRcv1SUg?showinfo=0&disablekb=1&loop=1&playlist=tNORRcv1SUg&controls=1&modestbranding=1\" frameborder=\"0\" allowfullscreen></iframe>\n",
              "</div>\n",
              "<div style=\"padding:10px 0px 0px 0px;\">\n",
              "<ul style=\"font-family: 'Roboto', 'Noto', sans-serif; font-size:16px; font-weight=500px; line-height:24px; -webkit-font-smoothing:'anti-aliased'; text-color:'--paper-grey-900'; align:'justify';\">\n",
              "<li> Generally, we use more number of neurons in layers closer to the input layer and gradually decrease the number of neurons as we move towards the output layer. Intuitively, this can be understood like this: <br>\n",
              "<ul>\n",
              "<li> More features means more detailed (or fine-grained) information about the input. At first, we compute very fine grained information from the input. And then, each successive layer summarizes information from the previous layer, and finally makes a prediction at the last layer. \n",
              "<li> Otherwise you can actually imagine it as making a lego house. First we compute very fine grained information about the data, which acts as the building blocks for next layer. Each hidden layer takes activations of the previous layer and build new informations.</br>\n",
              "<br>\n",
              "</ul>\n",
              "<i>(Note: There are a few exceptions to this general rule of thumb. E.g. bottleneck layers, U-net architecture)</i>\n",
              "<li> The number of layers, number of neurons in each layer, activation function for each layer, etc are collectively called as the architecture of the neural network.\n",
              "</ul>\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "<b>Additional footnnotes:</b><br>\n",
        "(The building blocks intuition is also in sync with the <a href=\"#scrollTo=Why_do_we_have_to_make_the_output_of_a_neuron_non_linear_\">intuition for using non-linear activations!)</a>"
      ],
      "metadata": {
        "id": "VJEP31SbPz9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some reasonable defaults for neural network architectures could also be:\n",
        "\n",
        "- If we have a smaller dataset, use a simple network - just 1 hidden layer.\n",
        "- More the number of hidden units, the better. (But, computationally, the neural network will become more expensive.)\n",
        "- If we are using more than 1 hidden layer, first try using the same number of hidden units in each hidden layer."
      ],
      "metadata": {
        "id": "R8rpyUeSB-u3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hypothesis\n",
        "- The above neural network takes in an input $\\small x = a^{(0)}$ of 4 features and predicts an output $ \\small h_\\Theta(x) = a^{(2)} $, the output of the last layer of the neural network. <br>\n",
        "In the above example, the value of the function $ \\small h_\\Theta(x) $ is a scalar, since there is only one neuron in the last layer.\n",
        "- $\\small h$ represents the entire function computed by the neural network, and is called the **hypothesis** of the neural network. <br>\n",
        " - Hypothesis, $\\small h $, is given by, <br>\n",
        " $$\n",
        " \\small h_\\Theta(x) = \\hat{y} \\tag{3}\n",
        " $$\n",
        " $ \\small \\text{where, } $\n",
        " $$\n",
        " \\small x \\text{, the input to the function represents one training example of our dataset, and} \\\\\n",
        "\\small \\hat{y} \\text{, represents the prediction for the given input, } \\\\\n",
        "\\small \\text{obtained using the parameters, } \\Theta\n",
        " $$\n",
        " <br>\n",
        " - In the above example of a 3 layer neural network, \n",
        "\\begin{align}\n",
        " \\small h_\\Theta(X) &= \\small \\hat{Y}  \\text{, the prediction for the entire training set} \\\\\n",
        " \\small &= \\small h_\\Theta(a^{(0)}) = a^{(2)}\\\\\n",
        " \\small &= \\small g^{[2]}((\\Theta^{(1)})^T \\: a^{(1)}) \\\\\n",
        " \\small &= \\small g^{[2]}((\\Theta^{(1)})^T \\:\\: g^{[1]}((\\Theta^{(0)})^T \\: a^{(0)}))\n",
        "\\end{align}\n",
        "   - Observe that, in a neural network, $\\small h_\\Theta(X)$ becomes a complex (or higher order) function and has a recurrance relationship on $\\small g^{[l]}$, the activation function of a layer.\n",
        "   - Unless we can solve this complex higher order reccurance of neural networks, it becomes impossible to vectorize the implementation of forward propagation. <br>\n",
        "   We *use an explicit for loop* to go through each layer and compute its activations, and then pass it to the next layer. In contrast, we have avoided the use of explicit for loops, with the help of vectorization, when computing th value of variables within a layer (e.g. $\\small z^{(l)}$, and $\\small a^{(l)}$)."
      ],
      "metadata": {
        "id": "WnpYPlxZti9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forward Propagation\n",
        "- The process of computing activations of the neurons starting from the first hidden layer to the last layer (i.e. output layer), is called **forward propagation.** \n",
        " - During forward propagation, each layer $\\small l$, takes the activations of the previous layer (along with its parameters, including the bias), and then computes its activation $\\small a^{(l)}$.\n",
        " - At the end of forward propagation, we end up with output of the last layer, $\\small a^{(l)} = \\hat{Y}$, output of the neural network.\n",
        " -In other words, we loop through layers $\\small l = 1, \\: 2, \\: ... \\:, \\: L$, to generate activations for each layer. <br>\n",
        "- The forward propagation may also be referred to as the forward pass.\n",
        "- The type of neural networks in which the sole input to each layer is the output of its immediate predecessor, are also called a **feed-forward neural networks**.\n",
        "- **Recurrent neural networks (RNN)** generally refer to the type of neural network architectures, where the input to a neuron can also include additional data input, along with the activation of the previous layer. E.g. for real-time handwriting or speech recognition.\n",
        "- **Residual neural networks (ResNet)** refer to another type of neural network architecture, where the input to a neuron can include the activations of two (or more) of its predecessors. E.g. for non-realtime handwriting or speech recognition.\n",
        " *(Note: RNNs and ResNets may be considered as equivalent, especially if we are performing non-real-time data analysis)*"
      ],
      "metadata": {
        "id": "ogqGMfA0twMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dimensional Analysis\n",
        "\n",
        "Fig. 3. represents a neural network with 3 layers. \n",
        "\n",
        "Let, <br>\n",
        " $\\small n$ represent the number of input features, <br>\n",
        " $\\small m$ represent the number of training examples, <br>\n",
        " $\\small l$ represent the $\\small l^{th}$ layer, <br>\n",
        " $\\small L$ represent the last layer,  and <br>\n",
        " $\\small S$ represent the number of neurons.\n",
        "\n",
        " Then, for the above neural network,\n",
        " - For the input layer, i.e. $\\small  l = 0, \\: \\: \\small S_l = S_0 = 4 = n $. \n",
        " - For the first hidden layer, $\\small  l = 1$, $\\small S_l = S_1 = 3 $, i.e, layer 1 computes 3 features using the input $\\small X = a^{(0)}$. \n",
        " - For the output layer, $\\small l = L = 2$ and $\\small S_l = S_L = S_2 = 1$, i.e. layer 2 computes one feature using $\\small \\text{activation of layer 1} = a^{(1)}$. <br>\n",
        " However, since layer $\\small L$ gives the output of the neural network, the feature computed by layer $\\small L$, i.e. $\\small a^{(2)}$ is called the prediction of the neural network.\n",
        "\n",
        "The parameters,\n",
        " <br>\n",
        " - $\\small \\Theta^{(0)}$ for $\\small l=0$, has the dimensions \n",
        " $\\small 5 \\times 3 = (S_0 + 1) \\times S_1 $.\n",
        " -  $\\small \\Theta^{(1)}$ for $\\small l=1$, has the dimensions $\\small 4 \\times 1 = (S_1 + 1) \\times S_2$. <br><br>\n",
        "  **Generalizing, dimensions of $\\small \\Theta^{(l)} $ can be given by:** <br>\n",
        "  $$\n",
        "  \\small \\boxed {\\text{Dim}(\\Theta^{(l)}) = (S_l + 1) \\times S_{l+1}} \\tag{4}\n",
        "  $$ <br>\n",
        "  *The + 1 term in the dimensions of $\\small \\Theta^{(l)}$ account for the additional row introduced so as to include bias.* \n",
        "\n",
        "The activations,\n",
        " - $\\small a^{(0)}$ for $\\small l=0$, has the dimensions \n",
        " $\\small 4 \\times m = S_0 \\times m $.\n",
        " - $\\small a^{(1)}$ for $\\small l=1$, has the dimensions \n",
        " $\\small 3 \\times m = S_1 \\times m $.\n",
        " - $\\small a^{(2)} = a^{(L)} = \\hat{Y}$ for $\\small l=L=2$, has the dimensions \n",
        " $\\small 1 \\times m = 1 \\times m $.\n",
        "<br><br>\n",
        "**Generalizing, dimensions of $\\small a^{(l)} $ can be given by:** <br>\n",
        "  $$\n",
        "  \\small \\boxed {\\text{Dim}(a^{(l)}) = S_l \\times m} \\tag{5}\n",
        "  $$ <br>\n",
        "  The $\\small \\times \\: m$, indicates that there is one column in the activation, for each training example. <br>\n",
        "  <br>\n",
        "  <i>Note that there is no</i> \" + 1 term\" <i>in the dimensions for $\\small a^{(l)}$. This is because $\\small a^{(l)}$ represents the activation of a layer; and the number of activations of a layer is equal to the number of neurons in the layer. Hence, $\\small a^{(l)}$ has dimensions $\\small S_l \\times m$. <br>\n",
        "  We introduce a row of 1's in $\\small a^{(l)}$ only during computation of activation <b>for the next layer</b>, which doesn't count as the activation of the layer. The row of 1's is simply a placeholder to enable matrix multiplication with $\\small \\Theta^{(l)}$.</i>\n",
        "\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "<b>Additional footnotes:</b><br>\n",
        "In the Machine Learning course by Prof. Andrew Ng on Coursera, instead of defining the parameter matrix $\\small \\Theta^{(l)}$ as a vertical stacking of parameters of each neuron in the layer, we are considering the parameter matrix as a horizontal stacking of parameters of each neuron, and thus avoids taking its transpose when computing $\\small z$. Hence, dimensions of $\\small \\Theta^{(l)}$, as per the lecture slides, is given by $\\small S_{l+1} \\times (S_l + 1)$, which is correct according to the definition of $\\small \\Theta$ used in the lectures."
      ],
      "metadata": {
        "id": "usdJ4vnF_4v6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example: 4 - layer neural network\n",
        "\n",
        "<div align=\"center\">\n",
        "<figure>\n",
        "  <img src=\"https://i.ibb.co/ssD4JYF/4-layer-neural-network-bias-and-weights-not-represented.png\">\n",
        "  <figcaption><b>Fig. 4. A neural network with 4 layers</b></figcaption>\n",
        "</figure>\n",
        "</div>\n",
        "\n",
        "The table below summarizes dimensions of the neural network shown above. <br><br>\n",
        "\\begin{array}{|c|c|}\\hline \n",
        " \\small \\text{Layer} & \\small \\text{Number of units} & \\small \\text{Parameter} & \\small\\text{Activation}\\\\\n",
        " \\small (l) & \\small (S_l) & \\small (\\Theta^{(l)}) & \\small (a^{(l)}) \\\\ \\hline\n",
        " \\small \\text{input layer, } 0   & \\small n=4 & \\small 5 \\times 3  & \\small 4 \\times m \\\\ \n",
        " \\small 1   & \\small 3   & \\small 4 \\times 2  & \\small 3 \\times m \\\\\n",
        " \\small 2   & \\small 2   & \\small 3 \\times 1  & \\small 2 \\times m \\\\ \n",
        " \\small L=3 & \\small 1   & \\small     -       & \\small 1 \\times m \\\\ \\hline\n",
        "\\end{array}\n"
      ],
      "metadata": {
        "id": "kzGnxATDCRMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementational note: Problem of symmetric weights\n",
        "\n",
        "- Do not initialize $\\small \\Theta$ to all zeros.  Then all neurons in one layer of the network will be computing the same value. If we have the same activation function for all layer, then all neurons in the entire neural  network will be computing  $\\small g(0)$.\n",
        "-  Do not initialize $\\small \\Theta$ to one constant value. Then all neurons in one layer of the network will be computing the same value. (Note: but now, different hidden layers will have different values.)\n",
        "- To avoid these problems, we have to break the symmetry; and for that, **always** use random initialization for $\\small \\Theta$.\n",
        " - To be computationally efficient, we do not want our initial weights to be very large. Therefore, we will use random initialization in a range $\\small [-\\epsilon, \\epsilon ]$ where $\\small \\epsilon$ is some small real value."
      ],
      "metadata": {
        "id": "59rM1iIH2nVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural networks for classification\n",
        "\n",
        "- So far, we have looked at neural networks that outputs only a single value. Such networks may be used for **regression** (predicting some real value e.g. housing prices, temperature of the day, etc.) or for binary classification.\n",
        "- A **binary classification** task means there are exactly two classes for the input data (lets say, A & B) and the input may belong to either of the two classes (but not both). <br>\n",
        "We can configure the neural network to predict either a 0 or a 1, to represent which class the input $\\small x$ belongs to. (E.g. *If output is 0, $\\small x \\in A$ and if output is 1, $\\small x \\notin A,$ i.e. $\\small x \\in B$*).\n",
        "- If we have more than two classes, (say A, B, C, D) and the input belongs to either one of the classes, then it is called a **multi-class classification** problem. <br> \n",
        "(Note that here also, we are dealing with disjoint classes, i.e an input can belong to only one of the classes.)\n",
        "- When an input can belong to more than one class, i.e. more than one class label is associated with one training example, then it is called a **multi-class, multi-label classification** problem.  \n",
        "- Given a training set: $ \\small \\{(x_1, y_1), (x_2, y_2), ... , (x_m, y_m)\\}$, <br> where $\\small x_i$ is one training example and $\\small y_i $ is its corresponding label. <br> The label $\\small y_i $ represents the actual class to which $\\small x_i$ belongs to.\n",
        " - For binary classification problems, $ \\small y_i $ can have either of the values 0 or 1, to denote which class the input belongs to.\n",
        " - For multi-class (and multi-label) classification problems, $ \\small y_i $ is represented as a one-hot vector of dimensions $ \\small K \\times 1 $, where $ \\small K$ is the number of classes(>2); i.e. if $ \\small y_{ik} = 0$, then $ \\small x_i \\notin k^{th}\\text{class} $ and if $ \\small y_{ik} = 1$, then $ \\small x_i \\in k^{th}\\text{class}$. <br> \n",
        "E.g. if \n",
        "$ \\small y_i = \n",
        "{\\begin{bmatrix} \n",
        "0\\\\ \n",
        "0\\\\ \n",
        "0\\\\ \n",
        "1\\\\ \n",
        "0 \n",
        "\\end{bmatrix}} $, \n",
        "then $ \\small x_i \\in 4^{th}\\text{ class}$, and there are $ \\small K$ = 5 classes.\n",
        "   - Note that, with this configuration, the set of all labels $\\small Y$ will be a sparse matrix of dimension $ \\small K \\times m$; one column for each training example and one row for each class.\n",
        "   - If it is a multi-class classification problem, there will be a 1 at exactly one index in $\\small y_i$.\n",
        "   - If it is a multi-class multi-label classification problem, there can be 1s at more than one index in $\\small y_i$.\n",
        "- To make use of neural networks for multi-class (and multi-label) classification problems, we let the neural network make more than one prediction at the output layer - one for each class. So if there are $ \\small K$ classes, then there will be $ \\small K$ neurons in the output layer of the neural network. <br>\n",
        "\n",
        "<div align=\"center\">\n",
        " <figure>\n",
        "  <img src=\"https://i.ibb.co/v1LTZ89/5-layer-NN-for-3-class-classification.png\">\n",
        "  <figcaption><b>Fig. 5. A 5-layer neural network for 3-class classification</b></figcaption>\n",
        "</figure>\n",
        "</div> \n",
        "\n",
        "- When using neural networks for a binary classification task, or for a multi-class multi-label classification task, the sigmoid activation function, $ \\small g(z) = \\frac{1}{1+e^{-z}} = \\sigma $, is a  good choice of activation function for the last layer (because the output of the sigmoid falls in the range between 0 and 1).\n",
        "- We usually use the softmax activation function, $ \\small g(z_i) = \\normalsize {\\frac{e^{z_i}}{\\sum_{j=1}^K{e^z_j}}} = \\small \\sigma(\\overrightarrow{z_i}) $ for the output layer, when performing a multi-class classification. The softmax activation function directly gives us the output in a vector form (one row of prediction for each class); whereas, if we are using sigmoid, we'll have to apply training as a generalization of one-vs-all classification for each output neuron. <br> (E.g. If we are using sigmoid, we have to manually train the first neuron to classify $\\small x \\in A$ or $\\small x \\notin A$, train the second neuron to classify $\\small x \\in B$ or $\\small x \\notin B$, etc. But with softmax, which takes in a vector as input, we can get a direct result without having to manually train the neurons for each class.)\n",
        "- Also note that, [cross-entropy loss functions](#scrollTo=Cross_Entropy_Loss) are another natural choice for classification tasks. \n",
        "\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "<b>Additional footnotes:</b><br>\n",
        "A sparse matrix is a matrix in which most of the entries are zeros. <br>\n",
        "<a href=\"https://stats.stackexchange.com/questions/207794/what-loss-function-for-multi-class-multi-label-classification-tasks-in-neural-n\">Related question in Stack Overflow</a\n"
      ],
      "metadata": {
        "id": "uKnHoT5uXi7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost function or Loss function, $ \\small J(\\Theta)$\n",
        "\n",
        "- At the end of a forward pass, we finally get the output of a neural network. We then compute the estimated cost of the neural network using the output of the last layer.\n",
        "- The term cost refers to how much the outputs of our machine learning model (here, the neural network) has deviated from the actual or expected outcomes.\n",
        "- In machine learning literature, the terms cost and loss are used synonymously, although there is a slight difference - the overall term of cost functions are positive, where as the overall term of loss functions are negative.\n",
        "- Intuitively, the term loss indicates how much useful information from the data got suppressed (or lost), during computation of the model output.\n",
        "- Different types of cost functions can be used for error estimation of different types of problems.\n",
        "- The cost function is denoted by $\\small J(\\Theta)$. <br>\n",
        " *(Loss functions may be denoted using $\\small L(\\Theta)$. However, these terms and notations can be used interchangeably.)*\n",
        " - Note that, input to the function is $\\small \\Theta$, the parameters of our model; which means the cost is sensitive to $\\small \\Theta$ and varies with $\\small \\Theta$, i.e. if we update the parameters of our model, then the cost will vary. \n",
        " - Hence, a suitable selection of parameters can help to significantly reduce the cost, and simultaneously make the model perform better (i.e give more accurate predictions).\n",
        " - Since cost function is sensitive to $\\small \\Theta$, its can be a useful tool for obtaining the best (otherwise, optimal) set of parameters for our model.\n",
        "\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "<b>Additional footnotes:</b> <br>\n",
        "The $\\small J$ in cost function is a reference to the <a hre\"https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant\">Jacobian</a>, because we use the Jacobian of the cost function to optimize it. <br>\n",
        "In simpler terms, Jacobian refers to the derivative of a function which has its output in the form of a matrix."
      ],
      "metadata": {
        "id": "_lKZtn8xjZps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost Optimization (or Model Training)\n",
        "\n",
        "- The task of minimizing the cost function over the set of our model parameters - in other words, choosing the best (or optimal) set of parameters for our model so that the cost function has the minimum possible value, is called **model training** and,<br> \n",
        "it is an optimization problem given by, <br>\n",
        "$$\n",
        "\\small \\min_\\Theta J(\\Theta) \\tag{6}\n",
        "$$\n",
        "- The derivative (or slope, or gradient) of a function gives a measure of how sensitive the function is to its input. <br>\n",
        "Since the cost function is sensitive to $\\small \\Theta$, the partial derivative of $\\small J$ with respect to $\\small \\Theta$, i.e. $\\small \\frac{\\partial J}{\\partial \\Theta}$ gives a measure of how a given cost function $\\small J$ varies when $\\small \\Theta$ varies.\n",
        "- For a given training example, an instance of the partial derivative $\\small \\frac{\\partial J}{\\partial \\Theta}$, i.e value of the derivative for a chosen set of values for the parameter $\\small \\Theta$, is a vector which indicates,\n",
        " - the direction : whether the error in the output of the model (for the given training example), increased or decreased, and \n",
        " - the magnitude : how much, or by what factor, the error changed, for the given training example; <br>\n",
        "when that particular choice of parameters was used.\n",
        "- Hence, we can use the measure, $\\small \\frac{\\partial J}{\\partial \\Theta}$, in determining how to update the parameters."
      ],
      "metadata": {
        "id": "knJgk7XbSg4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer and backpropagation\n",
        "\n",
        "- An **optimizer**, is an algorithm (or a function) that performs the task of optimizing the cost function, and thereby, it chooses the best set of values for the parameters of our model. <br>\n",
        "- An optimizer uses the partial derivative $\\small \\frac{\\partial J}{\\partial \\Theta}$, to determine the update criteria (the rules to update) for $\\small \\Theta$.\n",
        " - An optimizer is the function that performs model training.\n",
        " - An optimizer may iterate through the training set and update the parameters several times, to finally obtain the best choice of parameters for our model.\n"
      ],
      "metadata": {
        "id": "bi7x-9bwLf-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient of cost function of a neural network\n",
        "\n",
        "The parameters and biases, $\\small \\Theta^{(l)}$ and $\\small b^{(l)}$ are two independent variables, and hence, need different update criteria. Therefore, we find $\\small {\\frac{\\partial J}{\\partial \\Theta^{(l)}}}$ to update the weights, and $\\small {\\frac{\\partial J}{\\partial b^{(l)}}}$ to update the biases. For simplicity of derivation, let's consider them separately, such that $\\small z^{(l+1)} = {(\\Theta^{(l)})}^T a^{(l)} + b^{(l)}$. <br>\n",
        "(Note, $\\small b^{(l)}$ is not included as the first row in $\\small \\Theta^{(l)}$, for this discussion.)\n",
        "\n",
        "\\begin{align}\n",
        "\\small \\text{For layer } l \\neq L, &\\\\\n",
        "\\small \\text{From chain rule of derivatives, we get, } & &\\small \\Delta^{(l)} &= \\small {\\frac{\\partial J}{\\partial \\Theta^{(l)}} = \n",
        "\\frac{\\partial J}{\\partial a^{(l+1)}} \\: \\frac{\\partial a^{(l+1)}}{\\partial z^{(l+1)}} \\: \\frac{\\partial z^{(l+1)}}{\\partial \\Theta^{(l)}}} \\tag{7}  \\\\\n",
        "\\small \\text{We assign, } & &\n",
        "\\small \\delta^{(l)} &= \\small  \\frac{\\partial J}{\\partial a^{(l)}} \\: \\frac{\\partial a^{(l)}}{\\partial z^{(l)}} = \\frac{\\partial J}{\\partial z^{(l)}}  \\tag{8} \\\\ \n",
        "\\small  \\text{Note that, } & & \n",
        "\\small \\frac{\\partial a^{(l)}}{\\partial z^{(l)}} &= \n",
        "\\small  {g^{[l]}}'(z^{(l)}) \\\\\n",
        "\\small \\therefore & &\n",
        "\\small \\delta^{(l)} &= \\small  \\frac{\\partial J}{\\partial a^{(l)}} \\: {g^{[l]}}'(z^{(l)}) \\tag{9}\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "\n",
        "However, it turns out that we can use $\\small \\delta$ to update the bias terms also. \n",
        "\n",
        "\\begin{align}\n",
        "\\small \\frac{\\partial J}{\\partial b^{(l)}} &= \\small \\frac{\\partial J}{\\partial a^{(l+1)}} \\: \\frac{\\partial a^{(l+1)}}{\\partial z^{(l+1)}} \\: \\frac{\\partial z^{(l+1)}}{\\partial b^{(l)}} \\\\\n",
        "&= \\small \\frac{\\partial J}{\\partial a^{(l+1)}} \\: \\frac{\\partial a^{(l+1)}}{\\partial z^{(l+1)}} \\: \\frac{\\partial}{\\partial b^{(l)}} {(({\\Theta^{(l)})}^T \\: a^{(l)} + \\: b^{(l)})}\\\\\n",
        "&= \\small \\delta^{(l+1)} . \\: 1 & &\\small \\because  \\text{derivative of } \\Theta^{(l)} \\text{ w.r.t } b^{(l)} = 0\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "\n",
        "Note that, the last layer, i.e. layer $ \\small L$, does not have a matrix of parameters. Hence, <br><br>\n",
        "\\begin{align}\n",
        "\\small \\text{For layer } l = L, & &\n",
        "\\small \\Delta^{(L)} = {\\frac{\\partial J}{\\partial a^{(L)}} \\: \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} } = \\small  \\frac{\\partial J}{\\partial a^{(L)}} \\: {g^{[L]}}'(z^{(L)}) = \\delta^{(L)} \\tag{10}\n",
        "\\end{align}\n",
        "<br>\n",
        "**Notes:**\n",
        "1. We do not compute error for layer $ \\small l = 0 $, since it is the input layer.\n",
        "2. $\\small \\delta^{(l)} = \\frac{\\partial J}{\\partial z}$ is called the **error**, because it is an approximate measure of how sensitive the cost is to a particular training example.<br> \n",
        "\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "<b>Additional footnotes:</b><br>\n",
        "There is no superscript $\\small (l)$ for $\\small J$. Because, <br>\n",
        "$\\bullet$ cost is computed for the entire neural network at once, from the output of the last layer, i.e only the final output is taken into consideration for computing the cost and we do not compute cost for intermediate results (outputs of intermediate/hidden layers). Dimensionally, it is not possible to compute the cost for each hidden layer, because the hidden layers can have number of units which may or may not be equal to the required number of output features.<br>"
      ],
      "metadata": {
        "id": "ppQdA7r4yNqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Recurrence of $ \\small \\delta $ and $ \\small \\Delta $\n",
        "\n",
        "From equation $ \\small 8$, \n",
        "\\begin{align} \n",
        "\\small \\delta^{(l)} &=  \\small \\frac{\\partial J}{\\partial z^{(l)}} \\\\\n",
        "&= \\small \\frac{\\partial J}{\\partial z^{(l+1)}} \\: \\frac{\\partial z^{(l+1)}}{\\partial z^{(l)}}  \\\\\n",
        "&= \\small \\delta^{(l+1)} \\: \\frac{\\partial }{\\partial z^{(l)}}({(\\Theta^{(l)})}^T {g^{[l]}}(z^{(l)}) + \\: b^{(l)}) \\\\\n",
        "&= \\small \\delta^{(l+1)} \\: {(\\Theta^{(l)})}^T \\: {g^{[l]}}'(z^{(l)}) &\\small \\because  \\text{derivative of } b^{(l)} \\text{ w.r.t } \\Theta^{(l)} = 0 \\\\ \\\\\n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "\\Rightarrow \\boxed {\\delta^{(l)} = \\delta^{(l+1)} \\: {(\\Theta^{(l)})}^T \\: {g^{[l]}}'(z^{(l)})} & &  \\text{where, } l \\neq 0, \\: L\\tag{11}\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "\n",
        "From equation $ \\small 7$, \n",
        "\\begin{align} \n",
        "\\small \\Delta^{(l)} &=  \\small \\frac{\\partial J}{\\partial \\Theta^{(l)}} \\\\\n",
        "&= \\small {\\partial z^{(l+1)}} \\: \\frac{\\partial J}{\\partial \\Theta^{(l)}}\\: \\frac{1}{\\partial z^{(l+1)}}  \\\\\n",
        "&= \\small \\frac{\\partial z^{(l+1)}}{\\partial \\Theta^{(l)}} \\: \\frac{\\partial J}{\\partial z^{(l+1)}} \\\\\n",
        "&= \\small (\\frac{\\partial }{\\partial \\Theta^{(l)}} \\:({(\\Theta^{(l)})}^T \\: a^{(l)} + b^{(l)})) \\: \\delta^{(l+1)} \\\\ \n",
        "&= a^{(l)} \\: \\delta^{(l+1)} \\\\ \\\\\n",
        "\\end{align}\n",
        "\\begin{align}\n",
        "\\Rightarrow \\boxed {\\Delta^{(l)} = a^{(l)} \\: \\delta^{(l+1)}} & &  \\text{where, } l \\neq  L \\tag{12}\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "<b>Additional footnotes:</b><br>\n",
        "If we consider $ \\Delta^{(l)} =  \\frac{\\partial J}{\\partial \\Theta^{(l)}} = \\frac{\\partial J}{\\partial z^{(l+1)}}\\frac{\\partial z^{(l+1)}}{\\partial \\Theta^{(l)}}$, we will get $ \\Delta^{(l)} = \\delta^{(l+1)} a^{(l)}$. (Another implication: Both $\\delta^{(l+1)}$ and $a^{(l)}$ are simultaneously diagonalizable.)"
      ],
      "metadata": {
        "id": "P_wBM5z9AnOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Dimensional analysis for $ \\delta$ <br>\n",
        "\n",
        "$  {\\delta^{(l)} = \\delta^{(l+1)} \\: {(\\Theta^{(l)})}^T \\: {g^{[l]}}'(z^{(l)})} $\n",
        "\n",
        "---\n",
        "\n",
        "- $ \\small \\delta^{(l+1)}$ is a matrix of dimensions: $ \\small m \\times S_{l+1} $.\n",
        "- $ \\small \\Theta^{(l)}$ is a matrix of dimensions $ \\small S_{l} \\times S_{l+1}$. <br>\n",
        "- Therefore, $ \\: \\small {g^{[l]}}'(z^{(l)})$ *should be* a matrix of $ \\small S_{l} \\times S_{l}$. So that $ \\small \\delta^{(l)}$ is a matrix of dimensions: $ \\small m \\times S_{l}$.\n",
        "- It is observed that $ \\: \\small {g^{[l]}}'(z^{(l)})$ is a diagonal matrix, which means $ \\small \\: {g^{[l]}}'(z^{(l)})$ shrinks to a vector containing only its diagonal elements (rest are zeros). <br> \n",
        "(Intuitively, the dimensions $ \\small S_{l} \\times S_l $ seems absurd, because we do not need any correlation of weights of neurons within the same layer.) <br>\n",
        "  So, if we consider $ \\small \\: {g^{[l]}}'(z^{(l)})$ as a vector, i.e. $\\small \\overrightarrow{{g^{[l]}}'(z^{(l)})}$, then $ \\delta^{(l)} $ may also be represented as: <br> $  \\small {\\delta^{(l)} = \\delta^{(l+1)} \\: {(\\Theta^{(l)})}^T \\: \\odot (\\overrightarrow{{g^{[l]}}'(z^{(l)})}}) $\n",
        "<br><br>\n",
        "\n",
        "<b>Note:</b><br>\n",
        "<i>In my opinion</i>, $  \\small {\\delta^{(l)} = \\delta^{(l+1)} \\: {(\\Theta^{(l)})}^T \\: \\odot (\\overrightarrow{{{g^{[l]}}'(z^{(l)}}}})) $, may be understood as <br> $ \\small \\sum{(\\text{error of in the output of one unit, for one training example})(\\text{contribution of each weight towards causing the error})(\\text{influence of the input and its activation, responsible for the error})} $\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "<b>Additional footnotes:</b> <br>\n",
        "$\\small \\odot $ represents the element-wise product of two matrices or vectors.\n",
        "\n"
      ],
      "metadata": {
        "id": "9CQJRJURt2mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Dimensional analysis for $ \\Delta$ <br>\n",
        "\n",
        "$  \\Delta^{(l)} = a^{(l)} \\: \\delta^{(l+1)} $\n",
        "\n",
        "---\n",
        "\n",
        "- $ \\small a^{(l)}$ is a matrix of dimensions: $ \\small  S_{l} \\times m $.\n",
        "- $ \\small \\delta^{(l+1)}$ is a matrix of dimensions: $ \\small m \\times S_{l+1} $.\n",
        "- Thus, $ \\small \\Delta^{(l)} $ is a matrix of dimensions: $ \\small S_{l} \\times S_{l+1} $. \n",
        "<br><br>\n",
        "\n",
        "<b>Note:</b><br>\n",
        "<i>In my opinion</i>, $  \\small  \\Delta^{(l)} = a^{(l)} \\: \\delta^{(l+1)} $, may be understood as <br> $ \\small \\sum{(\\text{activation})(\\text{errors caused by that particular activation})} $\n",
        "\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "<b>Additional footnotes: </b><br>\n",
        "If we plug-in the formula for $\\small \\delta^{(l+1)}$ in $\\small \\Delta^{(l)}$, we get: <br> $\\small \\Delta^{(l)} = a^{(l)} \\: \\delta^{(l+2)} {(\\Theta^{(l+1)})}^T \\: {g^{[l+1]}}'(z^{(l+1)}) $ <br>\n",
        "$\\small \\implies  \\Delta^{(l)} $ is actually a double recurrence on $\\small \\delta$. <br> If we subsititute $\\small l=0$, we can see that $\\small \\Delta^{(0)}$ depends on $\\small \\delta^{(2)}$. <br>\n",
        "Hence, the minimum number of layers to call something a neural network, would be 3.\n"
      ],
      "metadata": {
        "id": "AoNBCEKy7WnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Derivative $\\small D$, of a neural network with a regularized cost function\n",
        "\n",
        "- Note that, $\\small \\Delta^{(l)} = a^{(l)}\\delta^{(l+1)}$, is a sum of products (since both are matrices). It is actually, a sum with $\\small m$ terms (see dimensions of $\\small a^{(l)}$ and $ \\small \\delta^{(l+1)}$), one for each training example, where each term gives a measure of how activation of one layer (for one training example) affected the next layer.<br>\n",
        "- Before applying any update to $\\small \\Theta$ based on $\\small \\Delta^{(l)}$, we take the average of these products. So we divide $\\small \\Delta^{(l)}$ with $\\small m$."
      ],
      "metadata": {
        "id": "Bisxw7g48_3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The problem of overfitting, and regularization\n",
        "- Usually neural networks are used when we have a large number of features. \n",
        "- However, if there are a large number of features (or we have a very complex hypothesis function), then our model will be prone to a problem called **overfitting**, i.e. the model performs well with data in the training set but cannot generalize, and fails (miserably) when it comes to predicting output for data not in the training set. \n",
        "- **Regularization** is one technique used to avoid overfitting. There are mainly [2 types of regularization](https://towardsdatascience.com/understanding-regularization-in-machine-learning-d7dd0729dde5):\n",
        "  1. L1 Regularization or Least Absolute Shrinkage and Selection Operator (LASSO) regression\n",
        "  2. L2 Regularization or Ridge Regression"
      ],
      "metadata": {
        "id": "je3UIOTZ-CSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### L2 Regularization or Ridge Regression\n",
        "   - The **L2 regularization** is the most commonly used in neural networks.\n",
        "   - To apply L2 regularization, we add an extra term to the cost function to compensate for having *too many features*. This extra term is given by, \n",
        "\\begin{align}\n",
        "\\small & & \\small \\frac{\\lambda}{2m}{{\\sum_{l=0}^{L}}{{\\sum_{i=1}^{S_{l}+1}}{{\\sum_{j=0}^{S_{l+1}}}{(\\Theta^{(l)}_{ji})^2}}}} \\tag{13} \\\\\n",
        "\\small \\text{where, } & & \\small \\lambda \\text{  is the regularization parameter.}\n",
        "\\end{align}\n",
        "   - Note that $\\small \\Theta^{(l)}_{ji}$ is indexed column-wise, this is just to indicate that parameters of each neuron is along the column, and we loop through parameters of one neuron at a time.\n",
        "   - Also, we do not apply regularization to the bias of the neurons, therefore the row index, $\\small i$ of $\\small \\Theta^{(l)}$ starts from 1 (not zero).\n",
        "   - Simply put, this term is a sum of squares of each individual parameter of the entire neural network, multiplied by a factor of $\\small \\frac{\\lambda}{2m}$.\n",
        "   - As you can see, this term (sum of squares) largely scales up the value of the cost function. Therefore, the weights will be heavily penalized (reduced) during optimization, i.e. with each update, the weights become smaller and smaller so that each neuron contributes only very little to the final output. (In other words, we are reducing the computational power of the entire neural network so that it doesn't learn *too much* and overfit the model.)\n",
        "   - Since neural networks are very much prone to overfitting, we use regularized cost functions to train the model. Hence, derivative of the cost function for a neural network (D) also includes the derivative of the regularization term with respect to $\\Theta$.\n",
        "   - The derivative of the L2 regularization term, for the entire neural network, is given by - \n",
        "   \\begin{align}\n",
        "   \\small \\frac{d}{d \\Theta}({ \\frac{\\lambda}{2m}{{\\sum_{l=0}^{L}}{{\\sum_{i=1}^{S_{l}+1}}{{\\sum_{j=0}^{S_{l+1}}}{(\\Theta^{(l)}_{ji})^2}}}})} &=  \\small { \\frac{\\lambda}{2m}{{\\sum_{l=0}^{L}}{{\\sum_{i=1}^{S_{l}+1}}{{\\sum_{j=0}^{S_{l+1}}}{\\frac{d}{d \\Theta}{(\\Theta^{(l)}_{ji})^2}}}}}} \\\\\n",
        "   \\small &= \\small { \\frac{\\lambda}{2m}{{\\sum_{l=0}^{L}}{{\\sum_{i=1}^{S_{l}+1}}{{\\sum_{j=0}^{S_{l+1}}}{2 \\: (\\Theta^{(l)}_{ji})}}}}} \\\\\n",
        "      \\small &= \\small { \\frac{\\lambda}{m}{{\\sum_{l=0}^{L}}{{\\sum_{i=1}^{S_{l}+1}}{{\\sum_{j=0}^{S_{l+1}}}{(\\Theta^{(l)}_{ji})}}}}} \\tag{14}\n",
        "   \\end{align}\n",
        "\n",
        "   - Therefore, for neural networks that uses L2 regularization, the derivative of the regularized cost function, D, for layer $\\small l$ is given by,\n",
        "   \\begin{align}\n",
        "   & & \\small D^{(l)} = \\frac{1}{m}{(\\Delta^{(l)} + \\lambda \\sum \\Theta^{(l)})} \\tag{15} \\\\\n",
        "   \\small \\text{where, } & & \\small \\lambda = 0, \\text{ for the first row (bias) in } \\Theta^{(l)}. \n",
        "   \\end{align}\n",
        "   \n"
      ],
      "metadata": {
        "id": "wpH0o6n0Q8mA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### L1 Regularization or Least Absolute Shrinkage and Selection Operator (LASSO) regression\n",
        "\n",
        "- In L1 regularization, we average over the absolute value of the parameters for each neuron , $\\small |\\Theta^{(l)}_j|$, and multiply it by a factor $\\small \\lambda$. \n",
        "- The L1 regularization term is given by - \n",
        "\\begin{align}\n",
        "& &\\small \\frac{\\lambda}{m}{\\sum^{L}_{l=1}{\\sum^{S_{l+1}}_{j = 0}}{|\\Theta^{(l)}_j|}} \\tag{16} \\\\ \n",
        "& \\small = & \\small \\frac{\\lambda}{m}{\\sum^{L}_{l=1}{\\sum^{S_{l+1}}_{j=0}{\\sqrt{\\sum^{S_{l}}_{i=1}{(\\Theta^{(l)}_{ji})^2}}}}}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "XeEgXyva-Ua0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different types of cost functions"
      ],
      "metadata": {
        "id": "U4O88zLVng3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean Squared Error Cost Function (MSE)\n",
        "\n",
        "- This is the simplest cost function.\n",
        "- First, we compute the error. The error is given by the difference between the predicted value and the expected outcome (actual value). <br>\n",
        "\\begin{align}\n",
        "\\small \\text{error in the output for one training example} &= \\small h_\\Theta(x) - y \\\\ \n",
        "\\small &= \\small \\hat{y} - y\n",
        "\\end{align}\n",
        "- Now, if we have $\\small m$ training examples, we find the error in the output for each training example, and then take the average of error<sup>2</sup>. <br>\n",
        "The mean squared error (MSE) cost function is given by,\n",
        "\\begin{align}\n",
        "\\small J(\\Theta) &= \\small \\frac{1}{m}\\sum^m_{i=1}{(\\hat{y}_i - y_i)^2} \\tag{17} \\\\\n",
        "&= \\small \\frac{1}{m}(\\hat{Y} - Y)^T(\\hat{Y} - Y)\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "<b>Additional Footnotes:</b><br>\n",
        "The vectorized implementation can be used only if both $\\small Y$ and $\\small \\hat{Y}$ are vectors, i.e. 1-dimensional."
      ],
      "metadata": {
        "id": "M-BKaiMm-WCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Root Mean Squared Error Cost Function (RMSE or RMS)\n",
        "\n",
        "- The RMS (Root Mean Square) cost function is a popular variation of the MSE cost function. It scales down the value of MSE by taking its square root.\n",
        "- The RMS cost function is given by, <br>\n",
        "\\begin{align}\n",
        "\\small J(\\Theta) &= \\sqrt{ \\frac{1}{m}\\sum^m_{i=1}{(\\hat{y}_i - y_i)^2} } \\tag{18} \\\\\n",
        "&= \\small \\sqrt{\\frac{1}{m}(\\hat{Y} - Y)^T(\\hat{Y} - Y)}\n",
        "\\end{align}\n",
        "\n",
        "- Both MSE and RMS are best suited for regression problems.\n"
      ],
      "metadata": {
        "id": "WygowSZhsH-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross Entropy Loss\n",
        "\n",
        "- For classification tasks, we usually use cross entropy loss functions.\n",
        "- The term entropy refers to randomness in a system (here, the input data); and the term cross entropy refers to randomness across two or more systems (here, different classes in the input data), i.e. how random is the probability that one element in a system (class) may be identified as belonging to another system (class).\n",
        "- If we have different classes (or categories) in the input data, and we want an estimate of how random is the probability that our model predicts the class of a training example correctly, we can use a cross entropy loss function.\n",
        "- Here also, we have to reduce the value of the loss function (reduce the entropy), so that our model makes predictions more precisely (and not randomly).\n",
        "<br><br>\n",
        "---\n",
        "<br>\n",
        "\n",
        "- A probability density function is a function that computes the probability of occurance of an event. (Here, the event will be - a given training example belonging to a particular class.) \n",
        "- Probability density functions always have a range of [0, 1] and the sum of probabilities always equal to 1. However, computation in this range becomes impractical, because each time we multiply probabilities, the values become closer to zero, and can become insignificantly small. For e.g. if 0.0000123 and 0.004212 are two probabilites, and we multiply them, we get the result as 0.0000000518076. <br>\n",
        " - To avoid this problem, we take the $\\small \\log$ of probabilities. <br> $\\small \\log(1) = 0$, <br>\n",
        "$\\small \\log(0) = \\text{undefined}$, but, we have $\\small \\lim_{x \\to 0}{(\\log(x))} = -\\infty$. <br>\n",
        "$\\small \\therefore \\text{by using log probabilities, essentially, we are rescaling probabilities to a more useful range of } (-\\infty, 0]. $\n",
        "However, since the values are negative, we multiply by -1 to make the probabilities positive.\n",
        "- To estimate cross entropies, we usually use $\\small \\log$ probabilities. Hence, the loss functions may also be referred to as log loss functions."
      ],
      "metadata": {
        "id": "DxhxaMN5llJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Binary Cross Entropy Loss Function\n",
        "\n",
        "- Suppose we have a class $\\small A$, and we are trying to predict whether a given training example $\\small x$ belongs to class A or not, i.e. the classification problem is to predict $\\small x \\in A \\text{ or } x \\notin A$. For error estimation of such problems, we can use the binary cross entropy loss function, given by <br>\n",
        "\n",
        "\\begin{align}\n",
        "\\small L(\\Theta) &= \\small \\frac{1}{m}{\\sum^m_{i=1}{-(y_i \\log \\hat{y}_i + (1-y_i) \\log (1-\\hat{y}_i))}} \\tag{19} \\\\\n",
        "\\small &= \\small \\frac{-1 \\:\\:}{m}{[Y^T \\log \\hat{Y} + (1-Y)^T \\log (1-\\hat{Y})]}\n",
        "\\end{align}\n",
        "<br>\n",
        "- Let's look at a single term in the loss function, $\\small l_i = -(y_i \\log \\hat{y}_i + (1-y_i) \\log (1-\\hat{y}_i)$) <br>\n",
        " - $\\small y_i$ is the actual probability that the i<sup>th</sup> training example belongs to class A. So, let's assign, $\\small y_i = A$.\n",
        " - Then, $\\small (1 - y_i)$ is the probability that the i<sup>th</sup> training example does not belong to class A, i.e. $\\small (1-y_i) = \\bar{A}$\n",
        " - $\\small \\hat{y_i}$ is the predicted probability that the i<sup>th</sup> training example belongs to class A. So, let's assign, $\\small \\hat{y_i} = P(A)$.\n",
        " - Then, $\\small (1 - \\hat{y_i})$ is the probability that the i<sup>th</sup> training example does not belong to class A, i.e. $\\small (1-\\hat{y_i}) = P(\\bar{A})$\n",
        " - The log is simply rescaling the probabilities to a more useful range. So, let's ignore it for now. And if we re-write the loss term, by taking the usual probabilities (not log probabilities), we can see that - <br> <br>\n",
        " \\begin{align}\n",
        " \\small l_i &= -(\\small y_i\\:\\: \\hat{y}_i & \\small + \\:\\: & \\small (1-y_i) \\:\\ (1-\\hat{y}_i)) \\\\\n",
        " &= -(\\underbrace{\\small A \\:. P(A)}  &  \\small + \\:\\:  & \\small \\: \\underbrace{\\bar A \\:. P(\\bar A)}) \\\\\n",
        "\\small &= -(\\small {P(\\text{being A and predicting A})} &\\small + \\:\\:  & \\small {P(\\text{being not A and predicting not A})}) \\\\ \\\\\n",
        "\\small &= \\small \\underline{\\underline{- P(\\text{making correct predictions})}} \\\\ \\\\\n",
        " \\end{align}\n",
        " $ \\small \\because \\text{we are actually taking log probabilities}, -P(x) \\text{ becomes its opposite.}$ <br>\n",
        "$\\\\ \\small \\text{( negative of log is its mirror image (i.e.opposite))} $ <br><br>\n",
        "\n",
        " \\begin{align}\n",
        " \\small \\implies \\boxed{ l_i = - P(\\text{making correct predictions}) \\\\\n",
        " \\small \\: = \\small P(\\text{making false predictions})  \\\\\n",
        " \\small \\: = \\small \\text{error for one training example} }\n",
        "\\end{align} <br>\n",
        "- Average of losses, $\\small l_i$, gives us the binary cross entropy loss function."
      ],
      "metadata": {
        "id": "gnkAp5-Pwh07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorical Cross Entropy Loss Function\n",
        "\n",
        "- When there are more than two classes, it is best to use a categorical cross entropy loss function. \n",
        "- The categorical cross entropy loss is just a generalization of the binary cross entropy loss, for $\\small n$ classes, and is given by - \n",
        "$$\n",
        "\\small L(\\Theta) = \\sum^{m}_{i=1}{\\sum^{K}_{k=1}{y_i \\log \\hat y_i}} \\tag{20}\n",
        "$$\n",
        "- Actually, if we look at the 2nd term of the binary cross entropy loss function, and let $\\small \\bar A $ be some other class $\\small B $, we get the categorical cross entropy loss for k=2 classes A and B."
      ],
      "metadata": {
        "id": "ZIx3S1Gvwsk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different types of optimizers"
      ],
      "metadata": {
        "id": "nRYte2Tbt2Xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent\n",
        "\n",
        "Suppose we have a convex cost function $\\small J$. A convex function is a function that has only one optimal value, i.e. it has only one minimum (or maximum).\n",
        "\n",
        "<div align=\"center\">\n",
        "  <figure>\n",
        "    <img src=\"https://3.bp.blogspot.com/-O1JqN3L0dPU/XJzTQP5yXnI/AAAAAAAAAxM/X6Xl3S4eDlgM0aEr-WXEK0OYWUynClpcgCLcBGAs/s640/respect_to_theta.png\" height=\"400px\"/>\n",
        "    <figcaption><b>Fig. 6. Example plot for a convex cost function</b></figcaption>\n",
        "  </figure>\n",
        "</div>\n",
        "\n",
        "For simplicity, let's say we have only one layer and one input feature, and its parameters be represented by $\\small \\Theta$. The above plot would represent the MSE cost function $\\small J(\\Theta)$, for the given input feature.\n",
        "\n",
        "Gradient descent is an optimization algorithm, which tries to move down the slope of the cost function, and hopefully reach the optimal value - the value of $\\small \\Theta$ that minimizes the value of cost function.\n",
        "\n",
        "<i> To move down the slope of the cost function, </i> the gradient descent algorithm computes the slope ($\\small \\frac{\\partial J}{\\partial \\Theta}$) and factors (or scales) it by multiplying with some constant $\\small \\alpha$, and then subtracts the (refactored) slope from the weights. The constant $\\small \\alpha$ is called the learning rate, because it is this constant that determines how fast or how slow the algorithm will move down slope; in other words, $\\small \\alpha$ is the constant that determines how fast the algorithm learns (or finds the best parameters for our model). \n",
        "\n",
        "The gradient descent update rule for weights, is given by: <br>\n",
        "\n",
        "\\begin{align}\n",
        "& \\small \\text{Repeat until convergence } \\{ &  & \\\\\n",
        "& & \\small \\Theta^{(l)} := \\Theta^{(l)} - \\alpha \\Delta^{(l)} \\tag{21} & \\\\\n",
        "& \\small \\}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Learning the best parameters, is also often referred to as **fitting the model**, which means we are trying to find (or fit) the best curve, or line - or simply, a function, that can best represent the patterns in the input data.\n",
        "\n",
        "Gradient descent for linear regression, for a single input feature $\\small \\Theta$ maybe visualized as follows. (Note: The notations are slightly different in the below figure.)\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://cdn-images-1.medium.com/max/800/1*KQVi812_aERFRolz_5G3rA.gif\" />\n",
        "  <figcaption><b>Fig. 7. Visualization of gradient descent for linear regression in one variable</b></figcaption>\n",
        "  <figcaption><a href=\"https://medium.com/machine-learning-101\">view source</a></figcaption>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "Now, let's say we have 2 input features, then the plot for $\\small J(\\Theta)$ would become 3-dimensional and optimizing a convex cost function would look like:\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://miro.medium.com/max/2400/1*gkl-HRUK35WejSqimAja1w.gif\" height=\"350\" width=\"auto\" />\n",
        "  <figcaption><b>Fig. 7. Visualization of gradient descent for linear regression in two variables</b></figcaption>\n",
        "  <figcaption><a href=\"https://medium.com/analytics-vidhya/cost-function-explained-in-less-than-5-minutes-c5d8a44b918c\">view source</a></figcaption>\n",
        "</div>\n",
        "\n",
        "Note: When there are more than 2 input features, it becomes hard to visualize, because each input feature adds one plane and we end up with planes of dimensions - 4D, 5D, 6D, ...\n",
        "\n",
        "\n",
        "This type of gradient descent, where the optimizer updates the parameters only after computing gradients of cost for all the training examples in the dataset, is called **batch gradient descent**."
      ],
      "metadata": {
        "id": "rrVX8sV_h1fk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Variations of gradient descent for non-convex cost functions\n",
        "\n",
        "A non-convex function is a function which can have multiple points of local minima.\n",
        "\n",
        "The following figure illustrates two possible trajectories (or paths) for gradient descent, on the surface of a non-convex cost function $\\small J$ (in 2 variables).\n",
        "\n",
        "<div align=\"center\">\n",
        "  <figure>\n",
        "    <img src=\"https://i.ibb.co/C7kLTGx/ezgif-5-64ac8aa068.gif\" height=\"300\"/>\n",
        "    <figcaption><b>Fig. 8. Two possible trajectories for gradient descent on the surface of a non-convex cost function in two variables</b></figcaption>\n",
        "  </figure>\n",
        "</div>\n",
        "\n",
        "When a function has more than one point of local optimum, then we cannot ensure that whether the (batch) gradient descent algorithm will always converge to the global minimum (the least value for the function - the least of all local minima points) for the cost function. The algorithm may as well get stuck in a local minimum point.\n",
        "\n",
        "In such situations, we use slightly modified versions of gradient descent, to ensure that the algorithm will always manage to converge to the global minimum for the function.\n",
        "\n",
        "The overall cost function for a neural network, is always non-convex. This is because there can be any number of permutations for the weights of neurons between different layers (or within the same layer), and hence, there can be multiple solutions (or multiple values of $\\small \\Theta$), that can lead to the same local minima.\n",
        "\n",
        "Hence, to optimize the cost in a neural network, we need some non-convex optimization techniques. Here are a few: <br>\n",
        "\n",
        "1. **Stochastic gradient descent (SGD)**: Stochastic means random. The stochastic gradient descent algorithm randomly picks 1 training example, computes the derivative of cost for that trainining example and then updates the weights (if the update would result in reaching a smaller value for cost than current value). Since each training example is randomly chosen, hopefully, we are avoiding the problem of being stuck on the same region of the plot. <br> \n",
        "However, the problem with SGD is that it may actually take a very much zigzagged (or noisy) path to convergence, and it can take a long time to execute since we update parameters after going through each training example. <br><br> <i>(Definitely better execution time than batch gradient descent though, in case of very large datasets. Say training set size, m = 100000000, then batch gradient descent will take an infinitely long time compared to SGD, because batch GD requires to go through all training examples, compute their gradient and then average over them, in order to update the parameters.)</i> <br> <br>\n",
        "\n",
        "2. **Mini-batch gradient descent**: In mini-batch gradient descent, we take a small subset of the training data, and update the weights after looking at all examples in the subset. Here, we *just assume that* there won't be no two points of local minima in any given subset (mini-batch), and gradient descent on each mini-batch would converge to the global minimum for that mini-batch.\n",
        "\n",
        "3. **Stochastic gradient descent with momentum**: In SGD with momentum, the algorithm remembers what change (or what update) was done during the previous iteration. Hence, when updating the parameter $\\small \\Theta$, it applies the usual SGD update, and then adds the value of the update during the preceeding iteration, refactored (i.e. multiplied) by an exponential decay factor $\\small \\beta$. The update rule for momentum is given by: \n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "<b> References:</b><br>\n",
        "1. <a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\">Wikipedia</a> <br>\n",
        "2. <a href=\"https://stats.stackexchange.com/questions/106334/cost-function-of-neural-network-is-non-convex/106343#106343?newreg=ff8ab7cef75d4fb5b749cc2e55c03f1e\">Cost function of neural network is non-convex</a><br>\n",
        "3. <a href=\"https://datascience.stackexchange.com/questions/36450/what-is-the-difference-between-gradient-descent-and-stochastic-gradient-descent\">What is the difference between gradient descent and stochastic gradient descent.</a>\n"
      ],
      "metadata": {
        "id": "GHNhQFYWiHyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Gradient descent intuitions\n",
        "\n",
        "<i>\n",
        "\n",
        "If we assume the surface of a non-convex cost function $\\small J$ as a plane with several hills and valleys, then \n",
        "- Gradient descent, or to be technically correct - **batch gradient descent**, can be considered as a man walking down the top of a hill, to finally reach the bottommost point in its valley.\n",
        "- **Stochastic gradient descent**, can be considered as airdropping a person at random points, to see if its the bottommost point in the valley.\n",
        "- **Mini-batch gradient descent**, can be considered as employing several men at different hill tops to walk down their valley, and then believe that one of them will most probably report at the bottommost point.\n",
        "- **Stochastic gradient descent with momentum** - Again, we are airdropping a man at random points, but now the man is a bit more wise and remembers which path he chose during the previous step and moves on based on that information. Thus, he usually stays on the right track, and takes a shorter and faster path to the bottommost point."
      ],
      "metadata": {
        "id": "2RXM4w2YWqJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix"
      ],
      "metadata": {
        "id": "biEPFNPE_oLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why do we have to make the output of a neuron non-linear?\n",
        "\n",
        "A neuron computes the linear function, $\\small z =  \\Theta^Tx + b $. Let's suppose we do not have a non-linear activation function.\n",
        "<div align=\"center\">\n",
        "  <figure>\n",
        "  <img src=\"https://i.ibb.co/bNL09Lj/linear-function.png\" />\n",
        "  <figcaption><b>Fig. 9. Feed-forward layers without non-linear activation functions  </b></figcaption>\n",
        "  </figure>\n",
        "</div>\n",
        "\n",
        "The neuron in its succeeding layer will be computing the same feature, and just scales up (or down) the magnitude of the feature. <br> Even if we add a third or 4th layer, the model learns nothing new, it keeps computing the same line it started with.\n",
        "\n",
        "However, if we add a slight non-linearity by using a non-linear activation function, for e.g. ReLU, $\\small g(z) = max(0, z)$, then the neuron in the succeeding layer will be computing a new feature (a different line).\n",
        "<div align=\"center\">\n",
        "  <figure>\n",
        "  <img src=\"https://i.ibb.co/zbhcJvx/non-linear-functions.png\" />\n",
        "  <figcaption><b>Fig. 10. Feed-forward layers with a non-linear activation function (eg. ReLU)  </b></figcaption>\n",
        "  </figure>\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "Now, the model can actually learn something new, rather than get stuck at computing the same feature over and over.\n",
        "If we add a third layer (in the 2nd image), the model will be able to learn a feature with 4 sides (quadrilateral), and so on.\n",
        "<br>\n",
        "---\n",
        "<br>\n",
        "<b>Additional footnotes:</b><br>\n",
        "1. <a href=\"(https://datascience.stackexchange.com/a/108450/126593\">Why is ReLU used as an activation function?</a> <br>\n",
        "2. <a href=\"https://datascience.stackexchange.com/a/108455/126593\">If ReLU is so close to being linear, why does it perform much better than a linear function?</a> <br>"
      ],
      "metadata": {
        "id": "dxQUX_fOzlve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List of all equations\n",
        "\n",
        "1. Input function, $\\small z^{(l)} = {(\\Theta^{(l-1)})}^Ta^{(l-1)}$, where 1st row of $\\small \\Theta^{(l)}$ comprises of all the bias terms.\n",
        "\n",
        "2. Activation, $\\small a^{(l)} = \\small  {g^{[l]}}(z^{(l)})$\n",
        "\n",
        "3. Hypothesis, $\\small h_\\Theta(x) = \\hat{y}$\n",
        "\n",
        "4. $\\small \\text{Dim}(\\Theta^{(l)}) = (S_l + 1) \\times S_{l+1}$\n",
        "\n",
        "5. $\\small \\text{Dim}(a^{(l)}) = S_l \\times m$. <br>\n",
        "<p align=\"right\">$\\small S_l$ represents number of neurons in layer $\\small l$.</p>\n",
        "\n",
        "6. Optimzation problem, $\\small \\min_\\Theta J(\\Theta)$\n",
        "\n",
        "7. Gradient of cost $\\small J$ with respect to $\\small \\Theta$, $\\small \\Delta^{(l)} = \\small {\\frac{\\partial J}{\\partial \\Theta^{(l)}} = \n",
        "\\frac{\\partial J}{\\partial a^{(l)}} \\: \\frac{\\partial a^{(l)}}{\\partial z^{(l)}} \\: \\frac{\\partial z^{(l)}}{\\partial \\Theta^{(l)}}}, \\:\\: l \\neq L$\n",
        "\n",
        "8. Gradient of cost $\\small J$ with respect to $\\small b$, $\\small \\delta^{(l)} = \\small  \\frac{\\partial J}{\\partial a^{(l)}} \\: \\frac{\\partial a^{(l)}}{\\partial z^{(l)}} = \\frac{\\partial J}{\\partial z^{(l)}}, \\:\\: l \\neq L$\n",
        "\n",
        "9. $\\small \\delta^{(l)} = \\small  \\frac{\\partial J}{\\partial a^{(l)}} \\: {g^{[l]}}'(z^{(l)}), \\:\\: l \\neq L$\n",
        "\n",
        "10. $\\small \\Delta^{(L)} = {\\frac{\\partial J}{\\partial a^{(L)}} \\: \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} } = \\small  \\frac{\\partial J}{\\partial a^{(L)}} \\: {g^{[L]}}'(z^{(L)}) = \\delta^{(L)}$\n",
        "\n",
        "11. Recurrence of $\\small \\delta^{(l)} = \\delta^{(l+1)} \\: {(\\Theta^{(l)})}^T \\: {g^{[l]}}'(z^{(l)}), \\:\\: l \\neq L$\n",
        "\n",
        "12. Recurrence of $\\small \\Delta^{(l)} = a^{(l)} \\: \\delta^{(l+1)}, \\:\\: l \\neq L$\n",
        "\n",
        "13. L2 regularization term, $\\small \\frac{\\lambda}{2m}{{\\sum_{l=0}^{L}}{{\\sum_{i=1}^{S_{l}+1}}{{\\sum_{j=0}^{S_{l+1}}}{(\\Theta^{(l)}_{ji})^2}}}}$\n",
        "\n",
        "14. Gradient of L2 regularization term, $\\small \\frac{d}{d \\Theta}({ \\frac{\\lambda}{2m}{{\\sum_{l=0}^{L}}{{\\sum_{i=1}^{S_{l}+1}}{{\\sum_{j=0}^{S_{l+1}}}{(\\Theta^{(l)}_{ji})^2}}}})} = \\small { \\frac{\\lambda}{m}{{\\sum_{l=0}^{L}}{{\\sum_{i=1}^{S_{l}+1}}{{\\sum_{j=0}^{S_{l+1}}}{(\\Theta^{(l)}_{ji})}}}}}$\n",
        "\n",
        "15. Gradient of neural network with L2 regularization, $\\small D^{(l)} = \\frac{1}{m}{(\\Delta^{(l)} + \\lambda \\sum\\Theta^{(l)})} \\text{ where, } \\small \\lambda = 0, \\text{ for the first row (bias) in } \\Theta^{(l)}$\n",
        "\n",
        "16. L1 regression term, $\\frac{\\lambda}{m}{\\sum^{L}_{l=1}{\\sum^{S_{l+1}}_{j = 0}}{|\\Theta^{(l)}_j|}} =  \\small \\frac{\\lambda}{m}{\\sum^{L}_{l=1}{\\sum^{S_{l+1}}_{j=0}{\\sqrt{\\sum^{S_{l}}_{i=1}{(\\Theta^{(l)}_{ji})^2}}}}}$\n",
        "\n",
        "17. Mean squared error cost function, $\\small J(\\Theta) = \\small \\frac{1}{m}\\sum^m_{i=1}{(\\hat{y}_i - y_i)^2} = \\small \\frac{1}{m}(\\hat{Y} - Y)^T(\\hat{Y} - Y)$\n",
        "\n",
        "18. Root mean squared error cost function, $\\small J(\\Theta) = \\sqrt{ \\frac{1}{m}\\sum^m_{i=1}{(\\hat{y}_i - y_i)^2} } = \\small \\sqrt{\\frac{1}{m}(\\hat{Y} - Y)^T(\\hat{Y} - Y)}$\n",
        "\n",
        "19. Binary cross entropy loss function, $\\small L(\\Theta) = \\small \\frac{1}{m}{\\sum^m_{i=1}{-(y_i \\log \\hat{y}_i + (1-y_i) \\log (1-\\hat{y}_i))}} = \\small \\frac{-1 \\:\\:}{m}{[Y^T \\log \\hat{Y} + (1-Y)^T \\log (1-\\hat{Y})]}$\n",
        "\n",
        "20. Categorical cross entropy loss function, $\\small L(\\Theta) = \\sum^{m}_{i=1}{\\sum^{K}_{k=1}{y_i \\log \\hat y_i}}$\n",
        "\n",
        "21. Gradient descent,\n",
        "\\begin{align}\n",
        "& \\small \\text{Repeat until convergence } \\{ &  & \\\\\n",
        "& & \\small \\Theta^{(l)} := \\Theta^{(l)} - \\alpha \\Delta^{(l)} \\tag{21} & \\\\\n",
        "& \\small \\}\n",
        "\\end{align}\n",
        "\n",
        "22. Stochastic gradient descent with momentum,\n",
        "\\begin{align}\n",
        "& \\small \\text{Repeat until convergence } \\{ &  & \\\\\n",
        "& & \\small \\Theta^{(l)} := \\Theta^{(l)} \\: \\: \\overbrace{- \\: \\alpha \\Delta^{(l)} + \\beta \\: \\underbrace{(\\scriptsize 𝝙 \\small \\Theta^{(l-1)})}_{\\text{update (𝝙) of previous layer} }}^{\\text{update 𝝙 for current layer}} & \\tag{22}\\\\\n",
        "& \\small \\}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "PFJkp9UQ_teT"
      }
    }
  ]
}